{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of PyTorch\n",
    "\n",
    "PyTorch is a popular open-source machine learning library that provides an efficient and flexible platform for building and training deep neural networks. Its key strength lies in its dynamic computational graph feature, which allows for on-the-fly modification and debugging of models. PyTorch is written in Python and provides a range of built-in functions and libraries for tasks such as image and text processing, time-series analysis, and more. The library's intuitive APIs and easy-to-learn syntax make it a popular choice among researchers and developers.\n",
    "\n",
    "**Installation**\n",
    "- [PyTorch.org](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a network \n",
    "To build a neural network in PyTorch, we can define a class that inherits from the `nn.Module` class. In the class, we'll define the layers of the network in the `__init__` function and define how data passes through the layers in the `forward` function. Here's an example of a neural network with two hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're defining a network with three layers: a fully connected layer with 784 input neurons and 256 output neurons, another one from 256 to 128 neurons, and an output layer that projects into 10 dimensions. The forward function takes an input x and passes it through each layer, applying the ReLU activation function to the output of the two hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential blocks\n",
    "For convenience, PyTorch allows us to define a neural network as a sequence of layers. We can use the `nn.Sequential` class for this purpose. Here's the same network as above using `nn.Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, tuning and evaluating a network\n",
    "Now that we have defined our neural network, we can train it on a dataset. To do so, we'll need to define a training loop and split the data into training, validation, and testing sets. Furthermore, we need to select a loss function and an optimizer.\n",
    "\n",
    "By splitting a dataset into training, validation, and testing sets, we can assess the performance of a model and ensure that the model is able to generalize well to unseen data. The adverse effect of just memorizing the training data is called overfitting.\n",
    "\n",
    "The training set is used to train the model's parameters, while the validation set is used to tune the model's hyperparameters, such as learning rate or regularization strength. The testing set is then used to evaluate the final performance of the model after it has been trained and fine-tuned. Optimally, the latter is only evaluated once, after the model has been trained and tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop iteratively optimizes a model to minimize the loss on the training data. At each iteration, a batch of data is passed through the model. The loss between the model's predictions and the true targets is then computed, and the gradients of the loss with respect to the model's parameters are backpropagated through the model. The optimizer uses these gradients to update the model's parameters in the direction of the negative gradient, using a specified update rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we're iterating through the training data in batches using a `train_loader`. For each batch, we're zeroing out the gradients of the optimizer, passing the inputs through the `model` to obtain the predicted outputs, and computing the loss between the predicted and target values (`criterion`). We then use the loss to compute the gradients of the model parameters using backpropagation, and update the parameters of the model using the `optimizer`'s step function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This whole process of going through the batched dataset will be repeated for a number of epochs or until convergence, with the goal of finding the optimal model parameters that minimize the loss on the training data. We will see this in the MNIST example below. For now it is only the inner loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation on the validation or even test set, we don't need to compute gradients, so we can set the model into evaluation mode `model.eval()` and wrap the code in a `torch.no_grad()` context manager to prevent PyTorch's autograd engine from tracking gradients. We can then pass the validation data through the model and compute the loss between the predicted and target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, targets).item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += preds.eq(targets).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on MNIST\n",
    "\n",
    "Now we can combine the model definition and training/evaluation strategy with a dataset. \n",
    "\n",
    "The MNIST dataset is a collection of 70,000 images of handwritten digits, each of size 28x28 pixels. It is a popular dataset for training and testing machine learning models for image classification tasks. (cf. http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "![MNIST Dataset](./MnistExamples.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model\n",
    "We can initialize the model that is conveniently defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the loss function and optimizer\n",
    "\n",
    "Next, we'll define the loss function and optimizer that we'll use to train the network. For the loss function, we'll use the cross-entropy loss, which is a common loss function for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root='./data_downloaded', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_data, _ = torch.utils.data.random_split(train_data, [50000, 10000], generator=torch.manual_seed(42))\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Standardization might be a bit overkill for an already 0-1 normalized MNIST.\n",
    "# However, pay attention that the parameters are only calculated on the training set.\n",
    "mean = 0.\n",
    "std = 0.\n",
    "for images, _ in train_loader:\n",
    "    num_samples = images.size(0)\n",
    "    images = torch.flatten(images, start_dim=1)\n",
    "    mean += images.mean(1).sum(0)\n",
    "    std += images.std(1).sum(0)\n",
    "mean /= len(train_loader.dataset)\n",
    "std /= len(train_loader.dataset)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Create datasets and loaders with new normalizing transforms\n",
    "train_data = datasets.MNIST(root='./data_downloaded', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data_downloaded', train=False, download=True, transform=transform)\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000], generator=torch.manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the full training loop for a number of epochs\n",
    "After every full cycle through the data, the performance is calculated on the validation set and can be reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Loss = 0.0026, Accuracy = 0.9503\n",
      "Epoch 6: Validation Loss = 0.0013, Accuracy = 0.9768\n",
      "Epoch 11: Validation Loss = 0.0014, Accuracy = 0.9789\n",
      "Epoch 16: Validation Loss = 0.0014, Accuracy = 0.9808\n",
      "Epoch 21: Validation Loss = 0.0015, Accuracy = 0.9802\n",
      "Epoch 26: Validation Loss = 0.0015, Accuracy = 0.9802\n",
      "Epoch 31: Validation Loss = 0.0016, Accuracy = 0.9803\n",
      "Epoch 36: Validation Loss = 0.0016, Accuracy = 0.9806\n",
      "Epoch 41: Validation Loss = 0.0016, Accuracy = 0.9802\n",
      "Epoch 46: Validation Loss = 0.0017, Accuracy = 0.9801\n",
      "Test on model epoch 50: Test Loss = 0.0013, Accuracy = 0.9831\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    train(model, train_loader, loss_function, optimizer)\n",
    "\n",
    "    # Evaluate the network on the validation set\n",
    "    val_loss, val_accuracy = infer(model, val_loader, loss_function)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Validation Loss = {val_loss:.4f}, Accuracy = {val_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the network on the test set\n",
    "test_loss, test_accuracy = infer(model, test_loader, loss_function)\n",
    "print(f\"Test on model epoch {epoch+1}: Test Loss = {test_loss:.4f}, Accuracy = {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing early stopping\n",
    "Early stopping is a technique to prevent overfitting. It involves monitoring a validation metric, such as the validation loss, during the training process and stopping the training early when the metric no longer improves. This is done by setting a threshold for the number of epochs or iterations with no improvement in the validation metric, `STOPPING_PATIENCE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPPING_PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Loss = 0.0024, Accuracy = 0.9515\n",
      "Epoch 2: Validation Loss = 0.0019, Accuracy = 0.9614\n",
      "Epoch 3: Validation Loss = 0.0015, Accuracy = 0.9708\n",
      "Epoch 4: Validation Loss = 0.0013, Accuracy = 0.9732\n",
      "Epoch 5: Validation Loss = 0.0014, Accuracy = 0.9726\n",
      "Epoch 6: Validation Loss = 0.0015, Accuracy = 0.9718\n",
      "Epoch 7: Validation Loss = 0.0015, Accuracy = 0.9731\n",
      "Epoch 8: Validation Loss = 0.0013, Accuracy = 0.9753\n",
      "Epoch 9: Validation Loss = 0.0013, Accuracy = 0.9754\n",
      "Epoch 10: Validation Loss = 0.0014, Accuracy = 0.9765\n",
      "Epoch 11: Validation Loss = 0.0014, Accuracy = 0.9770\n",
      "Epoch 12: Validation Loss = 0.0014, Accuracy = 0.9761\n",
      "Epoch 13: Validation Loss = 0.0013, Accuracy = 0.9780\n",
      "Epoch 14: Validation Loss = 0.0014, Accuracy = 0.9769\n",
      "Early stopping...\n",
      "Test on best model epoch 8, Test Loss = 0.0011, Accuracy = 0.9796\n"
     ]
    }
   ],
   "source": [
    "best_loss = np.inf\n",
    "best_epoch = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train(model, train_loader, loss_function, optimizer)\n",
    "\n",
    "    # Evaluate the network on the validation set\n",
    "    val_loss, val_accuracy = infer(model, val_loader, loss_function)\n",
    "    print(f\"Epoch {epoch+1}: Validation Loss = {val_loss:.4f}, Accuracy = {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Remainder only for early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    elif epoch - best_epoch > STOPPING_PATIENCE:\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "# Load the best model and evaluate it on the test set\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "test_loss, test_accuracy = infer(model, test_loader, loss_function)\n",
    "print(f\"Test on best model epoch {best_epoch+1}, Test Loss = {test_loss:.4f}, Accuracy = {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're training the network for up to `NUM_EPOCHS` epochs, and we're saving the model with the best test loss on the validation set. If the loss doesn't improve for more than 5 epochs, we stop training early to prevent overfitting. After this, we load the best model and test it on the test set to get an estimate of the general performance. The code also gives us an impression on how to do checkpointing (saving the parameters of a model every once in a while)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional neural network (or CNN) is a type of neural network comprising typically of following building blocks:\n",
    "- Convolutional Layer: These are a set of kernels/filters that convolve with a signal (1D: audio, EEG, etc; 2D: Images; 3D: Videos) to find particular patterns in it based on the kernel type. The kernels or filters are trainable.\n",
    "- Non-linearity: ReLU, Sigmoid, Tanh, etc.\n",
    "- Pooling layer: They downsample the input signal, which reduces the necessity to have a larger convolutional layer at the output. It also introduces a small translation invariance to the input signal.\n",
    "- Fully connected layer/Linear layer: They are mainly used at the end to model the actual decision process. Example: classifier.\n",
    "\n",
    "![Basic CNN block](cnn_architecture.svg)\n",
    "Image source: https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Loss = 0.0029, Accuracy = 0.9455\n",
      "Epoch 6: Validation Loss = 0.0021, Accuracy = 0.9614\n",
      "Epoch 11: Validation Loss = 0.0020, Accuracy = 0.9600\n",
      "Epoch 16: Validation Loss = 0.0023, Accuracy = 0.9558\n",
      "Epoch 21: Validation Loss = 0.0019, Accuracy = 0.9645\n",
      "Early stopping...\n",
      "Test on best model epoch 15, Test Loss = 0.0015, Accuracy = 0.9706\n"
     ]
    }
   ],
   "source": [
    "# Similar code as above\n",
    "best_loss = np.inf\n",
    "best_epoch = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train(model, train_loader, loss_function, optimizer)\n",
    "\n",
    "    # Evaluate the network on the validation set\n",
    "    val_loss, val_accuracy = infer(model, val_loader, loss_function)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Validation Loss = {val_loss:.4f}, Accuracy = {val_accuracy:.4f}\")\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), 'best_cnn_model.pth')\n",
    "    elif epoch - best_epoch > STOPPING_PATIENCE:\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "# Load the best model and evaluate it on the test set\n",
    "model.load_state_dict(torch.load('best_cnn_model.pth'))\n",
    "test_loss, test_accuracy = infer(model, test_loader, loss_function)\n",
    "print(f\"Test on best model epoch {best_epoch+1}, Test Loss = {test_loss:.4f}, Accuracy = {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `to` can be used to transfer PyTorch tensors to the GPU, where they can be processed using parallel computing. This will in most cases significantly speed up the training process. To define where to send the tensors, the string `cuda:` followed by the id of the gpu, e.g. `0`, can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, targets).item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += preds.eq(targets).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "\n",
    "# .to() on the model sends all parameters to device:\n",
    "model.to(device)\n",
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Loss = 0.0010, Accuracy = 0.9820\n",
      "Epoch 6: Validation Loss = 0.0007, Accuracy = 0.9889\n",
      "Epoch 11: Validation Loss = 0.0008, Accuracy = 0.9897\n",
      "Early stopping...\n",
      "Test on best model epoch 8, Test Loss = 0.0005, Accuracy = 0.9905\n"
     ]
    }
   ],
   "source": [
    "# Same code as above\n",
    "best_loss = np.inf\n",
    "best_epoch = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train(model, train_loader, loss_function, optimizer, device)\n",
    "\n",
    "    # Evaluate the network on the validation set\n",
    "    val_loss, val_accuracy = infer(model, val_loader, loss_function, device)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Validation Loss = {val_loss:.4f}, Accuracy = {val_accuracy:.4f}\")\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), 'best_cnn_model.pth')\n",
    "    elif epoch - best_epoch > STOPPING_PATIENCE:\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "# Load the best model and evaluate it on the test set\n",
    "model.load_state_dict(torch.load('best_cnn_model.pth'))\n",
    "test_loss, test_accuracy = infer(model, test_loader, loss_function, device)\n",
    "print(f\"Test on best model epoch {best_epoch+1}, Test Loss = {test_loss:.4f}, Accuracy = {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a custom dataset\n",
    "Make sure to check out the other notebook `deep_learning_start.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, stimuli_dir, fs, bandpass_band, split=\"train\"):\n",
    "        self.data_dir = data_dir\n",
    "        self.stimuli_dir = stimuli_dir\n",
    "        self.split = split\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load raw or preprocessed data here\n",
    "        # Splitting can also be done before\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            pass\n",
    "        elif self.split == \"val\":\n",
    "            pass\n",
    "        elif self.split == \"test\":\n",
    "            pass\n",
    "        \n",
    "        # Attended signal could be 'left' or 'right'. We can achieve this in different ways.\n",
    "        if np.random.random() < 0.5:\n",
    "            return eeg_split, attended_split, unattended_split, torch.tensor([0], dtype=torch.double)\n",
    "        else:\n",
    "            return eeg_split, unattended_split, attended_split, torch.tensor([1], dtype=torch.double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
